import asyncio
import datetime
import logging
from typing import Optional

import discord

from src.cogs.handlers.tool_selector import ToolSelector
from src.cogs.handlers.rag_handler import RAGHandler
from src.utils.core_client import core_client

logger = logging.getLogger(__name__)


class ChatHandler:
    def __init__(self, cog):
        self.cog = cog
        self.bot = cog.bot
        self.tool_selector = ToolSelector(self.bot)
        self.rag_handler = RAGHandler(self.bot)
        logger.info("ChatHandler v3.9.2 (RAG Enabled) Initialized")

    async def handle_prompt(
        self,
        message: discord.Message,
        prompt: str,
        existing_status_msg: Optional[discord.Message] = None,
        is_voice: bool = False,
        force_dm: bool = False,
    ) -> None:
        """
        [Thin Client] Process a user message by delegating to ORA Core.
        Discord handles UI (Status, Voice, Embeds) while Core handles Brain.
        """
        # 1. Initialize StatusManager and Request Tracking
        from src.utils.ui import EmbedFactory, StatusManager
        import uuid
        correlation_id = str(uuid.uuid4())
        logger.info(f"ðŸ†• [Chat] New Request | CorrelationID: {correlation_id} | User: {message.author.id}")

        status_manager = StatusManager(message.channel, existing_message=existing_status_msg)
        await status_manager.start("ðŸ“¡ ORA Core Brain ã¸æŽ¥ç¶šä¸­...")

        # 2. Determine Context Binding
        kind = "channel"
        ext_id = f"{message.guild.id}:{message.channel.id}" if message.guild else f"dm:{message.author.id}"

        if not message.guild:
            kind = "dm"
        elif hasattr(message.channel, "parent_id") and message.channel.parent_id:
            kind = "thread"
            ext_id = f"{message.guild.id}:{message.channel.parent_id}:{message.channel.id}"

        context_binding = {
            "provider": "discord",
            "kind": kind,
            "external_id": ext_id
        }

        try:
            # 2.5 Build Rich Client Context for Brain
            client_context = {
                "timestamp": datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "server_name": message.guild.name if message.guild else "Direct Message",
                "guild_id": str(message.guild.id) if message.guild else None,
                "channel_id": str(message.channel.id),
                "channel_name": message.channel.name if hasattr(message.channel, "name") else "DM",
                "is_admin": message.author.guild_permissions.administrator if message.guild else True
            }

            # 3. Call Core API
            # [MEMORY INJECTION] Fetch User Profile
            memory_context = ""
            try:
                memory_cog = self.cog.bot.get_cog("MemoryCog")
                if memory_cog:
                    # Use a timeout to prevent hanging if file lock issue
                    user_profile = await asyncio.wait_for(
                        memory_cog.get_user_profile(message.author.id, message.guild.id if message.guild else None),
                        timeout=1.0
                    )

                    if user_profile:
                        # Extract key info
                        name = user_profile.get("name", message.author.display_name)
                        impression = user_profile.get("impression", "None")
                        traits = ", ".join(user_profile.get("traits", []))

                        memory_context = f"""
[USER PROFILE]
Name: {name}
Impression: {impression}
Traits: {traits}
"""
            except Exception as e:
                logger.warning(f"Memory Fetch Failed: {e}")

            # [SOURCE INJECTION] Explicitly state this is Discord
            # [Moltbook] Inject Soul (Persona) if available
            soul_injection = getattr(self.cog, "soul_prompt", "")
            if soul_injection:
                soul_injection = f"\n[SYSTEM IDENTITY]\n{soul_injection}\n"

            # [DEVICE AWARENESS]
            is_mobile = False
            if message.guild and isinstance(message.author, discord.Member):
                if message.author.is_on_mobile():
                    is_mobile = True

            system_context = f"""
{soul_injection}
[ã‚½ãƒ¼ã‚¹: DISCORD]
[ã‚µãƒ¼ãƒãƒ¼: {message.guild.name if message.guild else 'Direct Message'}]
[ãƒãƒ£ãƒ³ãƒãƒ«: {message.channel.name if hasattr(message.channel, 'name') else 'DM'}]

[ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæŒ‡ä»¤: Codex Harness ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£]
ã‚ãªãŸã¯ OpenAI Codex Harness ã«åŸºã¥ãè‡ªå¾‹åž‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ã™ã€‚ã€ŽEverything is controlled by codeã€ã®åŽŸå‰‡ã«å¾“ã„ã€å…¨ã¦ã®æ“ä½œã‚’ã€Žã‚¹ã‚­ãƒ«ï¼ˆSkillï¼‰ã€ã¨ã—ã¦åˆ¶å¾¡ã—ã¦ãã ã•ã„ã€‚

- **Agentic Search (è‡ªå¾‹åž‹æŽ¢ç´¢)**: RAG ãªã©ã®å›ºå®šçš„ãªã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«é ¼ã‚‹ã®ã§ã¯ãªãã€`code_grep`, `code_find`, `code_read`, `code_tree` ã¨ã„ã£ãŸã‚¹ã‚­ãƒ«ã‚’èƒ½å‹•çš„ã«ä½¿ã„ã€ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã‚„ãƒ‡ãƒ¼ã‚¿ã‚’ç›´æŽ¥èª¿æŸ»ã—ã¦ãã ã•ã„ã€‚
- **vibes (æ„Ÿè¦š) ã§ã®åˆ¤æ–­**: ãƒ™ã‚¯ã‚¿ãƒ¼æ¤œç´¢ã®çµæžœã‚ˆã‚Šã‚‚ã€ã‚ãªãŸãŒå®Ÿéš›ã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¦‹ã¦æ–‡è„ˆï¼ˆContextï¼‰ã‚’ç†è§£ã—ã€åˆ¤æ–­ã™ã‚‹ã“ã¨ã‚’å„ªå…ˆã—ã¦ãã ã•ã„ã€‚
- **æœ€å¼·ã®è¦–è¦šèƒ½åŠ›**: ã‚ãªãŸã¯æä¾›ã•ã‚ŒãŸ `image_url` ã‚’ç›´æŽ¥è§£æžã§ãã¾ã™ã€‚è¦–è¦šæƒ…å ±ã‚’å‰æã¨ã—ãŸé«˜åº¦ãªæŽ¨è«–ã‚’è¡Œã£ã¦ãã ã•ã„ã€‚
- **4Kå¯¾å¿œ**: é«˜ç”»è³ªè¦æ±‚ã«ã¯ `resolution: "4K"` ã‚’æŒ‡å®šã€‚
- **è‡ªå·±è¦–è¦šãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯**: å®Ÿè¡Œã—ãŸã‚¹ã‚­ãƒ«ã®æˆæžœï¼ˆã‚¹ã‚¯ã‚·ãƒ§ç­‰ï¼‰ã¯å³åº§ã«ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã•ã‚Œã¾ã™ã€‚

[ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ—ãƒ­ãƒˆã‚³ãƒ«: å®Ÿè¡Œè¨ˆç”»ã®è¡¨ç¤º]
è¤‡é›‘ãªæ‰‹é †ãŒå¿…è¦ãªå ´åˆã€è¿”ç­”ã®å†’é ­ã«ã€ŒðŸ“‹ **ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè¡Œè¨ˆç”» (Skill Plan)**:ã€ã‚’æç¤ºã—ã¦ãã ã•ã„ã€‚

[Harness Event Protocol]
ã‚ãªãŸã®æ€è€ƒï¼ˆThoughtï¼‰ã¨é€²æ—ï¼ˆProgressï¼‰ã¯ã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§ Harness ã‚¹ãƒˆãƒªãƒ¼ãƒ ã¸é€å‡ºã•ã‚Œã¾ã™ã€‚

[é‹ç”¨ãƒ«ãƒ¼ãƒ«: CAPTCHA / Anti-Bot]
ãƒ–ãƒ©ã‚¦ã‚¶æ“ä½œä¸­ã« CAPTCHA ã‚„ã€ŒI'm not a robot / unusual trafficã€ãªã©ã®æ¤œçŸ¥ãŒå‡ºãŸå ´åˆã€
å›žé¿ãƒ»çªç ´ã‚’è©¦ã¿ã¦ã¯ã„ã‘ã¾ã›ã‚“ã€‚ä»£ã‚ã‚Šã«æ¬¡ã®æ–¹é‡ã§è‡ªå¾‹çš„ã«è§£æ±ºã—ã¦ãã ã•ã„:
1) ç›´æŽ¥ãƒ–ãƒ©ã‚¦ã‚¶æ¤œç´¢ã‚’åœæ­¢
2) `web_search` / `read_web_page` ãªã©APIç³»ãƒ„ãƒ¼ãƒ«ã¸åˆ‡æ›¿
3) å¿…è¦ãªã‚‰ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«æ‰‹å‹•ç¢ºèªã‚’ä¾é ¼ã—ã€ç¢ºèªå¾Œã«æ¬¡ã®ã‚¿ã‚¹ã‚¯ã¸é€²ã‚€

[ãƒ‡ãƒã‚¤ã‚¹æƒ…å ±]
{"[MOBILE] ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯ãƒ¢ãƒã‚¤ãƒ«ç«¯æœ«ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚å›žç­”ã¯ç°¡æ½”ã«ã¾ã¨ã‚ã€è¤‡é›‘ãªè¡¨ã‚„ãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆã¯é¿ã‘ã¦ãã ã•ã„ã€‚" if is_mobile else "[DESKTOP] ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯PCã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚è©³ç´°ãªè§£èª¬ã¨ãƒªãƒƒãƒãªãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆãŒå¯èƒ½ã§ã™ã€‚"}

{memory_context}
"""

            # Prepend to prompt
            full_prompt = system_context.strip() + "\n\n" + prompt

            # [Vision Integration] Process Attachments & References
            vision_suffix = ""
            image_payloads = []

            try:
                # 1. Current Message
                # 1. Current Message
                # PERF: Unified GPT-5 Environment. Direct Image payload is sent.
                # We skip the captioning suffix to avoid redundant LLM calls and latency.
                if message.attachments:
                    # Only collect bytes/base64, don't trigger describe_media
                    _, imgs = await self.cog.vision_handler.process_attachments(message.attachments)
                    image_payloads.extend(imgs)

                # 2. Referenced Message (Reply) context
                if message.reference:
                    try:
                        if message.reference.cached_message:
                            ref_msg = message.reference.cached_message
                        else:
                            ref_msg = await message.channel.fetch_message(message.reference.message_id)

                        if ref_msg:
                            full_prompt += f"\n\n[REPLYING TO MESSAGE (Author: {ref_msg.author.display_name})]:\n{ref_msg.content or '(No Text)'}"
                            for embed in ref_msg.embeds:
                                if embed.url: full_prompt += f"\n[EMBED URL]: {embed.url}"

                            # Vision for References
                            if ref_msg.attachments:
                                 suffix, imgs = await self.cog.vision_handler.process_attachments(ref_msg.attachments, is_reference=True)
                                 vision_suffix += suffix
                                 image_payloads.extend(imgs)

                            if ref_msg.embeds:
                                 suffix, imgs = await self.cog.vision_handler.process_embeds(ref_msg.embeds, is_reference=True)
                                 vision_suffix += suffix
                                 image_payloads.extend(imgs)

                    except Exception as e:
                        logger.warning(f"Failed to fetch referenced message: {e}")

            except Exception as e:
                logger.error(f"Vision Processing Failed: {e}")
                # Fallback: Continue without vision data rather than crashing
                full_prompt += "\n[SYSTEM ERROR: Image processing failed. Proceeding with text only.]"

            # Append Vision Text Context
            full_prompt += vision_suffix

            # Prepare attachments for LLM (UnifiedClient expects 'attachments' list of dicts)
            # The structure from VisionHandler is already compatible or needs minor adapt?
            # UnifiedClient.chat expects 'attachments' argument.
            # But here we are building `messages` manually?
            # Wait, `endpoints.py` handles `attachments` argument.
            # In `ChatHandler`, we delegate to `core_client` or `unified_client`.

            # Update: ChatHandler calls `core_client.submit_message`.
            # We need to pass `image_payloads` to clean attachments.
            # Currently `chat_handler.py` uses `self.cog.unified_client` or `core_client`.
            # Let's see the call site further down.

            # Looking at previous code, `attachments` variable was created:
            # attachments = []
            # for att in message.attachments: ...

            # So I should assign `attachments = image_payloads`

            attachments = image_payloads

            # Send Request (Initial Handshake)
            # Fetch Context-Aware Tools (Discord Only)
            discord_tools = self.cog.get_context_tools("discord")

            # [RAG ROUTER] Analyze Intent & Select Tools
            # This reduces context usage and improves accuracy
            await status_manager.update_current("ðŸ” Intent Analysis (RAG)...")

            # [Clawdbot Feature] Vector Memory Retrieval (User + Guild Shared)
            guild_id_str = str(message.guild.id) if message.guild else None
            rag_context = await self.rag_handler.get_context(
                prompt=prompt,
                user_id=str(message.author.id),
                guild_id=guild_id_str
            )

            # Append RAG context to system prompt or user prompt?
            # Ideally User prompt to make it visible to the model as "Context"
            full_prompt_with_rag = f"{rag_context}\n{full_prompt}"

            # Select tools based on Platform Context
            selected_tools = await self.tool_selector.select_tools(
                prompt=prompt,
                available_tools=discord_tools,
                platform="discord",
                rag_context=rag_context,
                correlation_id=correlation_id
            )

            # If router judges the request complex, force explicit plan-first behavior.
            route_meta = getattr(self.tool_selector, "last_route_meta", {}) or {}
            if route_meta.get("complexity") == "high":
                full_prompt_with_rag = (
                    "[ORCHESTRATION POLICY: COMPLEX TASK]\n"
                    "ã“ã®ä¾é ¼ã¯è¤‡é›‘ã§ã™ã€‚å¿…ãšæœ€åˆã«ã€ŽðŸ“‹ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè¡Œè¨ˆç”»ã€ã‚’çŸ­ãæç¤ºã—ã¦ã‹ã‚‰ã€"
                    "å¿…è¦ãªãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã‚’é–‹å§‹ã—ã¦ãã ã•ã„ã€‚\n\n"
                ) + full_prompt_with_rag

            # If tools were filtered, log it
            if len(selected_tools) != len(discord_tools):
                logger.info(f"Tool Selection: {len(discord_tools)} -> {len(selected_tools)} tools")

            bot_cfg = getattr(self.bot, "config", None)
            preferred_model = getattr(bot_cfg, "openai_default_model", "gpt-5-mini")

            response = await core_client.send_message(
                content=full_prompt_with_rag,
                provider_id=str(message.author.id),
                display_name=message.author.display_name,
                conversation_id=None,
                idempotency_key=f"discord:{message.id}",
                context_binding=context_binding,
                attachments=attachments,
                stream=False, # User requested no streaming
                client_context=client_context,
                available_tools=selected_tools,  # Use RAG selected tools
                source="discord",
                llm_preference=preferred_model,
                correlation_id=correlation_id
            )

            if "error" in response:
                await status_manager.finish()
                await message.reply(f"âŒ Core API æŽ¥ç¶šã‚¨ãƒ©ãƒ¼: {response['error']}")
                return

            run_id = response.get("run_id")
            await status_manager.update_current(f"ðŸ§  ORA Brain ãŒè€ƒãˆä¸­... (ID: {run_id[:8]})")

            # 4. Process SSE Events (Streaming/Incremental Updates)
            full_content = ""
            model_name = "ORA Universal Brain"
            download_summaries = []
            tool_feedback_summaries = []
            if hasattr(self, "_plan_sent"):
                del self._plan_sent

            async for event in core_client.stream_events(run_id):
                ev_type = event.get("event")
                ev_data = event.get("data", {})

                if ev_type == "delta":
                    full_content += ev_data.get("text", "")

                    # [VISUALIZATION] Check if content is an Execution Plan (Relaxed Match)
                    has_plan_header = "Execution Plan" in full_content or "å®Ÿè¡Œè¨ˆç”»" in full_content
                    if has_plan_header and "1." in full_content and not hasattr(self, "_plan_sent"):
                        # Only send ONCE per run
                        msg_lines = full_content.split("\n")
                        plan_lines = [line.strip() for line in msg_lines if line.strip().startswith("1.") or line.strip().startswith("2.") or line.strip().startswith("3.") or line.strip().startswith("-")]

                        if plan_lines:
                             embed = discord.Embed(
                                 title="ðŸ¤– Harness Agent Execution Plan",
                                 description="\n".join(plan_lines),
                                 color=0x00ffff # Cyan (Codex Style)
                             )
                             embed.set_footer(text="OpenAI Codex Harness Architecture")
                             await message.reply(embed=embed)
                             self._plan_sent = True

                elif ev_type == "thought":
                    # Stream thoughts to a separate log or specific UI element
                    thought_text = ev_data.get("text", "")
                    logger.info(f"ðŸ§  [Harness Thought] {thought_text[:100]}...")

                elif ev_type == "progress":
                    # Update status bar with Harness Progress
                    status_text = ev_data.get("status", "")
                    await status_manager.update_current(f"âš¡ [Harness] {status_text}")

                elif ev_type == "meta":
                     model_name = ev_data.get("model", model_name)

                elif ev_type == "dispatch":
                    # TOOL CALL detected!
                    tool_name = ev_data.get("tool")
                    tool_args = ev_data.get("args", {})
                    tool_call_id = ev_data.get("tool_call_id")
                    logger.info(f"ðŸš€ [Dispatch] CID: {correlation_id} | Tool: {tool_name}")

                    # Call ToolHandler (Handles music, imagine, tts, etc.)
                    # We pass the message context so it knows where to reply or join voice.
                    # [FIX] Use await instead of create_task to ensure SEQUENTIAL execution.
                    # This is critical for chains like "Screenshot -> Download -> Screenshot".
                    tool_result = await self.cog.tool_handler.handle_dispatch(
                        tool_name=tool_name,
                        args=tool_args,
                        message=message,
                        status_manager=status_manager,
                        correlation_id=correlation_id
                    )

                    if isinstance(tool_result, dict):
                        dl_meta = tool_result.get("download_meta")
                        if isinstance(dl_meta, dict):
                            summary = dl_meta.get("assistant_summary")
                            if isinstance(summary, str) and summary.strip():
                                download_summaries.append(summary.strip())
                        result_txt = tool_result.get("result")
                        if isinstance(result_txt, str) and result_txt.strip():
                            cleaned = result_txt.replace("[SILENT_COMPLETION]", "").strip()
                            if cleaned:
                                tool_feedback_summaries.append(cleaned)
                    elif isinstance(tool_result, str):
                        cleaned = tool_result.replace("[SILENT_COMPLETION]", "").strip()
                        if cleaned:
                            tool_feedback_summaries.append(cleaned)

                    # [FIX/AGENTIC] Submit Tool Result back to Core to break deadlock
                    if run_id:
                        logger.info(f"ðŸ“¤ Auto-submitting tool output for {tool_name} to Core...")
                        await core_client.submit_tool_output(
                            run_id=run_id,
                            tool_name=tool_name,
                            result=tool_result or "[Success]",
                            tool_call_id=tool_call_id,
                        )

                elif ev_type == "final":
                    final_text = ev_data.get("text", "")
                    if isinstance(final_text, str) and final_text.strip():
                        full_content = final_text
                    model_name = ev_data.get("model", model_name)
                    break

                elif ev_type == "error":
                    await status_manager.finish()
                    await message.reply(f"âš ï¸ Core Error: {ev_data.get('message', 'Unknown error')}")
                    return

            # 5. Final Output Handover
            try:
                # [FIX] Flush any buffered files (Smart Bundling)
                if status_manager and hasattr(status_manager, "flush_files"):
                    try:
                        await status_manager.flush_files(message)
                    except Exception as e:
                        logger.error(f"Failed to flush files: {e}")
                        await message.reply(f"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«é€ä¿¡ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            finally:
                # Always finish status manager
                await status_manager.finish()

            if not full_content and not response.get("run_id"): # If we had tools, content might be empty but OK
                await message.reply("âŒ å¿œç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
                return

            # If Core generated only a generic dispatch sentence, replace it with concrete download metadata summary.
            if download_summaries:
                generic_markers = [
                    "ä¿å­˜å‡¦ç†ã‚’discordã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã«ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒ",
                    "ä¿å­˜ãŒå®Œäº†ã—ãŸã‚‰",
                    "ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒã—ã¾ã—ãŸ",
                ]
                low = (full_content or "").lower()
                if (not full_content.strip()) or any(m in low for m in generic_markers):
                    full_content = "\n".join(download_summaries[-2:])

            # General fallback: if core final is empty, surface concrete tool feedback.
            if not (full_content or "").strip() and tool_feedback_summaries:
                uniq = []
                for t in tool_feedback_summaries:
                    if t not in uniq:
                        uniq.append(t)
                full_content = "\n".join(uniq[-2:])

            if not (full_content or "").strip():
                full_content = "ãƒ„ãƒ¼ãƒ«å‡¦ç†ã¯å®Ÿè¡Œã•ã‚Œã¾ã—ãŸãŒã€æœ€çµ‚ãƒ†ã‚­ã‚¹ãƒˆå¿œç­”ãŒç©ºã§ã—ãŸã€‚å¿…è¦ãªã‚‰çµæžœã‚’å†è¡¨ç¤ºã—ã¾ã™ã€‚"

            # Send as Embed Cards
            # Split if > 4000 chars
            remaining = full_content
            while remaining:
                chunk = remaining[:4000]
                remaining = remaining[4000:]
                embed = EmbedFactory.create_chat_embed(chunk, model_name=model_name)
                await message.reply(embed=embed)

            # 6. Post-Process Actions (Voice, etc.)
            # [MEMORY UPDATE] Inject AI response into MemoryCog buffer
            try:
                memory_cog = self.bot.get_cog("MemoryCog")
                if memory_cog:
                    asyncio.create_task(
                        memory_cog.add_ai_message(
                            user_id=message.author.id,
                            content=full_content,
                            guild_id=message.guild.id if message.guild else None,
                            channel_id=message.channel.id,
                            channel_name=message.channel.name if hasattr(message.channel, "name") else "DM",
                            guild_name=message.guild.name if message.guild else "Direct Message",
                            is_public=message.author.guild_permissions.administrator if message.guild else True # Use same logic as context
                        )
                    )
            except Exception as e:
                logger.warning(f"Failed to update MemoryCog: {e}")

            # Check if user is in VC and if we should speak
            if is_voice:
                # [REQUESTED FEATURE] Suppress AI speech if this channel is an Auto-Read channel
                # User wants "Reading Bot" behavior where AI text response is just text, unless specifically asked?
                # Or simply "Don't read AI response".
                should_speak = True
                if message.guild and hasattr(self.bot, "voice_manager"):
                    auto_channel_id = self.bot.voice_manager.auto_read_channels.get(message.guild.id)
                    if auto_channel_id == message.channel.id:
                        should_speak = False

                # [FIX] Accessed via bot instance as it's a shared resource now
                if should_speak and hasattr(self.bot, "voice_manager"):
                    await self.bot.voice_manager.play_tts(message.author, full_content)
                else:
                    logger.warning("VoiceManager not found on Bot instance.")

        except Exception as e:
            logger.error(f"Core API Delegation Failed: {e}", exc_info=True)
            await status_manager.finish()
            await message.reply(f"ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼: {e}")

    # --- END OF THIN CLIENT ---
