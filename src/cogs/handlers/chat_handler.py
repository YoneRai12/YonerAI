import asyncio
import datetime
import json
import logging
from typing import Optional

import discord

from src.cogs.handlers.tool_selector import ToolSelector
from src.cogs.handlers.rag_handler import RAGHandler
from src.cogs.handlers.swarm_orchestrator import SwarmOrchestrator
from src.utils.agent_trace import trace_event
from src.utils.core_client import core_client, extract_text_from_core_data

logger = logging.getLogger(__name__)


class ChatHandler:
    def __init__(self, cog):
        self.cog = cog
        self.bot = cog.bot
        self.tool_selector = ToolSelector(self.bot)
        self.rag_handler = RAGHandler(self.bot)
        self.swarm = SwarmOrchestrator(self.bot, self.tool_selector.llm_client)
        logger.info("ChatHandler v3.9.2 (RAG Enabled) Initialized")

    @staticmethod
    def _sanitize_args_for_audit(args: dict) -> str:
        """Mask sensitive keys and keep payload short for Discord audit posts."""
        sensitive_markers = ("token", "secret", "password", "api_key", "authorization", "cookie")

        def scrub(value):
            if isinstance(value, dict):
                out = {}
                for k, v in value.items():
                    lk = str(k).lower()
                    if any(m in lk for m in sensitive_markers):
                        out[k] = "[REDACTED]"
                    else:
                        out[k] = scrub(v)
                return out
            if isinstance(value, list):
                return [scrub(v) for v in value]
            return value

        safe = scrub(args or {})
        text = json.dumps(safe, ensure_ascii=False)
        return text[:700] + ("..." if len(text) > 700 else "")

    async def _notify_agent_activity(self, title: str, description: str, color: int = 0x2B6CB0) -> None:
        cfg = getattr(self.bot, "config", None)
        if not cfg:
            return
        target_id = getattr(cfg, "feature_proposal_channel_id", None) or getattr(cfg, "log_channel_id", None)
        if not target_id:
            return
        channel = self.bot.get_channel(target_id)
        if not channel:
            try:
                channel = await self.bot.fetch_channel(target_id)
            except Exception:
                return
        if not channel or not hasattr(channel, "send"):
            return
        # Discord embed title hard limit is 256 chars.
        safe_title = (title or "").strip()
        if len(safe_title) > 256:
            safe_title = safe_title[:253] + "..."
        embed = discord.Embed(title=safe_title or "ORA", description=description[:3900], color=color)
        embed.timestamp = discord.utils.utcnow()
        try:
            await channel.send(embed=embed)
        except Exception as e:
            logger.debug(f"Agent activity notify skipped: {e}")

    async def handle_prompt(
        self,
        message: discord.Message,
        prompt: str,
        existing_status_msg: Optional[discord.Message] = None,
        is_voice: bool = False,
        force_dm: bool = False,
    ) -> None:
        """
        [Thin Client] Process a user message by delegating to ORA Core.
        Discord handles UI (Status, Voice, Embeds) while Core handles Brain.
        """
        # 1. Initialize StatusManager and Request Tracking
        from src.utils.ui import EmbedFactory, StatusManager
        import uuid
        correlation_id = str(uuid.uuid4())
        logger.info(f"ðŸ†• [Chat] New Request | CorrelationID: {correlation_id} | User: {message.author.id}")
        trace_event(
            "chat.request_received",
            correlation_id=correlation_id,
            user_id=str(message.author.id),
            guild_id=str(message.guild.id) if message.guild else None,
            channel_id=str(message.channel.id),
            prompt=prompt,
        )

        status_manager = StatusManager(message.channel, existing_message=existing_status_msg)

        # Dynamic task board: do not hardcode 3 steps for every request.
        # Keep it short for simple chats; expand only when the request implies multi-step work.
        p_low = (prompt or "").lower()
        tasks = ["ä¾é ¼ã‚’è§£æž"]
        if message.attachments:
            tasks.append("æ·»ä»˜ã‚’è§£æž")
        if any(k in p_low for k in ["ãƒ­ã‚°", "trace", "ã‚¨ãƒ©ãƒ¼", "stack", "ä¾‹å¤–"]):
            tasks += ["ãƒ­ã‚°/çŠ¶æ³ã‚’ç¢ºèª", "åŽŸå› ã‚’ç‰¹å®š", "ä¿®æ­£æ¡ˆã‚’æç¤º"]
        elif any(k in p_low for k in ["ä¿å­˜", "ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", "download", "save", "mp3", "mp4", "å‹•ç”»"]):
            tasks += ["ä¿å­˜/ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œ", "çµæžœã‚’æ•´ç†"]
        elif any(k in p_low for k in ["ã‚¹ã‚¯ã‚·ãƒ§", "ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆ", "screenshot", "webã²ã‚‰ã„ã¦", "webæ“ä½œ", "ãƒ–ãƒ©ã‚¦ã‚¶"]):
            tasks += ["ãƒšãƒ¼ã‚¸ã‚’é–‹ã", "ã‚¹ã‚¯ã‚·ãƒ§/æ“ä½œã‚’å®Ÿè¡Œ"]
        # Always end with a reply step.
        tasks.append("å›žç­”ã‚’è¿”ã™")

        # De-dup + clamp
        seen = set()
        tasks = [t for t in tasks if not (t in seen or seen.add(t))]
        tasks = tasks[:8]

        await status_manager.start_task_board(
            "âš¡ ORA Universal Brain â€¢ å®Ÿè¡Œã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹",
            tasks,
            footer="Sanitized & Powered by ORA Universal Brain",
        )
        await status_manager.set_task_state(1, "running", "Coreã¸æŽ¥ç¶šä¸­")

        # 2. Determine Context Binding
        kind = "channel"
        ext_id = f"{message.guild.id}:{message.channel.id}" if message.guild else f"dm:{message.author.id}"

        if not message.guild:
            kind = "dm"
        elif hasattr(message.channel, "parent_id") and message.channel.parent_id:
            kind = "thread"
            ext_id = f"{message.guild.id}:{message.channel.parent_id}:{message.channel.id}"

        context_binding = {
            "provider": "discord",
            "kind": kind,
            "external_id": ext_id
        }

        try:
            # 2.5 Build Rich Client Context for Brain
            from src.utils.access_control import is_owner

            client_context = {
                "timestamp": datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "server_name": message.guild.name if message.guild else "Direct Message",
                "guild_id": str(message.guild.id) if message.guild else None,
                "channel_id": str(message.channel.id),
                "channel_name": message.channel.name if hasattr(message.channel, "name") else "DM",
                # ORA "admin" means creator/owner (ADMIN_USER_ID), not guild permissions.
                "is_admin": is_owner(self.bot, message.author.id),
            }

            # 3. Call Core API
            # [MEMORY INJECTION] Fetch User Profile
            memory_context = ""
            channel_memory_context = ""
            guild_memory_context = ""
            try:
                memory_cog = self.cog.bot.get_cog("MemoryCog")
                if memory_cog:
                    # Use a timeout to prevent hanging if file lock issue
                    user_profile = await asyncio.wait_for(
                        memory_cog.get_user_profile(message.author.id, message.guild.id if message.guild else None),
                        timeout=1.0
                    )

                    if user_profile:
                        # Extract key info
                        name = user_profile.get("name", message.author.display_name)
                        impression = user_profile.get("impression", "None")
                        traits = ", ".join(user_profile.get("traits", []))
                        l2 = user_profile.get("layer2_user_memory", {}) if isinstance(user_profile, dict) else {}
                        facts = ""
                        interests = ""
                        try:
                            if isinstance(l2, dict):
                                facts = "; ".join([str(x) for x in (l2.get("facts") or []) if str(x).strip()])[:800]
                                interests = "; ".join([str(x) for x in (l2.get("interests") or []) if str(x).strip()])[:400]
                        except Exception:
                            pass

                        memory_context = f"""
[USER PROFILE]
Name: {name}
Impression: {impression}
Traits: {traits}
Facts: {facts}
Interests: {interests}
"""

                    # Channel-level memory (summary/topics/atmosphere)
                    try:
                        ch_profile = await asyncio.wait_for(
                            memory_cog.get_channel_profile(message.channel.id),
                            timeout=1.0,
                        )
                        if isinstance(ch_profile, dict) and ch_profile:
                            c_sum = (ch_profile.get("summary") or "").strip()
                            c_atm = (ch_profile.get("atmosphere") or "").strip()
                            c_topics = ch_profile.get("topics") or []
                            if not isinstance(c_topics, list):
                                c_topics = []
                            c_topics_s = ", ".join([str(x) for x in c_topics if str(x).strip()])[:200]

                            lines = []
                            if c_sum:
                                lines.append(f"- Summary: {c_sum[:500]}")
                            if c_topics_s:
                                lines.append(f"- Topics: {c_topics_s}")
                            if c_atm:
                                lines.append(f"- Atmosphere: {c_atm[:120]}")

                            if lines:
                                channel_memory_context = "\n[CHANNEL MEMORY]\n" + "\n".join(lines) + "\n"
                    except Exception:
                        pass

                    # Guild/server-level memory (high-level server identity / dominant topics)
                    try:
                        if message.guild and hasattr(memory_cog, "get_guild_profile"):
                            g_profile = await asyncio.wait_for(
                                memory_cog.get_guild_profile(message.guild.id),
                                timeout=1.0,
                            )
                            if isinstance(g_profile, dict) and g_profile:
                                g_hint = (g_profile.get("hint") or "").strip()
                                g_topics = g_profile.get("topics") or []
                                if not isinstance(g_topics, list):
                                    g_topics = []
                                g_topics_s = ", ".join([str(x) for x in g_topics if str(x).strip()])[:250]
                                lines = []
                                if g_hint:
                                    lines.append(f"- Hint: {g_hint[:500]}")
                                if g_topics_s:
                                    lines.append(f"- Topics: {g_topics_s}")
                                if lines:
                                    guild_memory_context = "\n[GUILD MEMORY]\n" + "\n".join(lines) + "\n"
                    except Exception:
                        pass

                    # Light heuristic: if the creator/sub-admin explicitly states server identity
                    # (e.g., "ã“ã“ã¯VALORANTã®é¯–"), persist it as a guild hint to bias acronym disambiguation.
                    try:
                        from src.utils.access_control import is_owner, is_sub_admin
                        if message.guild and hasattr(memory_cog, "set_guild_hint"):
                            txt = (message.content or "").strip()
                            low = txt.lower()
                            if (("ã“ã®é¯–" in txt) or ("ã“ã®ã‚µãƒ¼ãƒ" in txt) or ("ã“ã“ã¯" in txt)) and any(k in low for k in ["valorant", "valo", "ãƒãƒ­", "ãƒãƒ­ãƒ©ãƒ³ãƒˆ"]):
                                if is_owner(self.bot, message.author.id) or is_sub_admin(self.bot, message.author.id):
                                    await memory_cog.set_guild_hint(
                                        message.guild.id,
                                        "This server is primarily VALORANT-related (Valorant-focused context).",
                                    )
                    except Exception:
                        pass
            except Exception as e:
                logger.warning(f"Memory Fetch Failed: {e}")

            # [SOURCE INJECTION] Explicitly state this is Discord
            # [Moltbook] Inject Soul (Persona) if available
            soul_injection = getattr(self.cog, "soul_prompt", "")
            if soul_injection:
                soul_injection = f"\n[SYSTEM IDENTITY]\n{soul_injection}\n"

            # [DEVICE AWARENESS]
            is_mobile = False
            if message.guild and isinstance(message.author, discord.Member):
                if message.author.is_on_mobile():
                    is_mobile = True

            system_context = f"""
 {soul_injection}
 [ã‚½ãƒ¼ã‚¹: DISCORD]
 [ã‚µãƒ¼ãƒãƒ¼: {message.guild.name if message.guild else 'Direct Message'}]
 [ãƒãƒ£ãƒ³ãƒãƒ«: {message.channel.name if hasattr(message.channel, 'name') else 'DM'}]

[ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæŒ‡ä»¤: Codex Harness ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£]
ã‚ãªãŸã¯ OpenAI Codex Harness ã«åŸºã¥ãè‡ªå¾‹åž‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ã™ã€‚ã€ŽEverything is controlled by codeã€ã®åŽŸå‰‡ã«å¾“ã„ã€å…¨ã¦ã®æ“ä½œã‚’ã€Žã‚¹ã‚­ãƒ«ï¼ˆSkillï¼‰ã€ã¨ã—ã¦åˆ¶å¾¡ã—ã¦ãã ã•ã„ã€‚

- **Agentic Search (è‡ªå¾‹åž‹æŽ¢ç´¢)**: RAG ãªã©ã®å›ºå®šçš„ãªã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«é ¼ã‚‹ã®ã§ã¯ãªãã€`code_grep`, `code_find`, `code_read`, `code_tree` ã¨ã„ã£ãŸã‚¹ã‚­ãƒ«ã‚’èƒ½å‹•çš„ã«ä½¿ã„ã€ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã‚„ãƒ‡ãƒ¼ã‚¿ã‚’ç›´æŽ¥èª¿æŸ»ã—ã¦ãã ã•ã„ã€‚
- **vibes (æ„Ÿè¦š) ã§ã®åˆ¤æ–­**: ãƒ™ã‚¯ã‚¿ãƒ¼æ¤œç´¢ã®çµæžœã‚ˆã‚Šã‚‚ã€ã‚ãªãŸãŒå®Ÿéš›ã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¦‹ã¦æ–‡è„ˆï¼ˆContextï¼‰ã‚’ç†è§£ã—ã€åˆ¤æ–­ã™ã‚‹ã“ã¨ã‚’å„ªå…ˆã—ã¦ãã ã•ã„ã€‚
- **æœ€å¼·ã®è¦–è¦šèƒ½åŠ›**: ã‚ãªãŸã¯æä¾›ã•ã‚ŒãŸ `image_url` ã‚’ç›´æŽ¥è§£æžã§ãã¾ã™ã€‚è¦–è¦šæƒ…å ±ã‚’å‰æã¨ã—ãŸé«˜åº¦ãªæŽ¨è«–ã‚’è¡Œã£ã¦ãã ã•ã„ã€‚
- **4Kå¯¾å¿œ**: é«˜ç”»è³ªè¦æ±‚ã«ã¯ `resolution: "4K"` ã‚’æŒ‡å®šã€‚
- **è‡ªå·±è¦–è¦šãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯**: å®Ÿè¡Œã—ãŸã‚¹ã‚­ãƒ«ã®æˆæžœï¼ˆã‚¹ã‚¯ã‚·ãƒ§ç­‰ï¼‰ã¯å³åº§ã«ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã•ã‚Œã¾ã™ã€‚

[ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ—ãƒ­ãƒˆã‚³ãƒ«: å®Ÿè¡Œè¨ˆç”»ã®è¡¨ç¤º]
è¤‡é›‘ãªæ‰‹é †ãŒå¿…è¦ãªå ´åˆã€è¿”ç­”ã®å†’é ­ã«ã€ŒðŸ“‹ **ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè¡Œè¨ˆç”» (Skill Plan)**:ã€ã‚’æç¤ºã—ã¦ãã ã•ã„ã€‚

[Harness Event Protocol]
ã‚ãªãŸã®æ€è€ƒï¼ˆThoughtï¼‰ã¨é€²æ—ï¼ˆProgressï¼‰ã¯ã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§ Harness ã‚¹ãƒˆãƒªãƒ¼ãƒ ã¸é€å‡ºã•ã‚Œã¾ã™ã€‚

 [é‹ç”¨ãƒ«ãƒ¼ãƒ«: CAPTCHA / Anti-Bot]
 ãƒ–ãƒ©ã‚¦ã‚¶æ“ä½œä¸­ã« CAPTCHA ã‚„ã€ŒI'm not a robot / unusual trafficã€ãªã©ã®æ¤œçŸ¥ãŒå‡ºãŸå ´åˆã€
 å›žé¿ãƒ»çªç ´ã‚’è©¦ã¿ã¦ã¯ã„ã‘ã¾ã›ã‚“ã€‚ä»£ã‚ã‚Šã«æ¬¡ã®æ–¹é‡ã§è‡ªå¾‹çš„ã«è§£æ±ºã—ã¦ãã ã•ã„:
 1) ç›´æŽ¥ãƒ–ãƒ©ã‚¦ã‚¶æ¤œç´¢ã‚’åœæ­¢
 2) `web_search` / `read_web_page` ãªã©APIç³»ãƒ„ãƒ¼ãƒ«ã¸åˆ‡æ›¿
 3) å¿…è¦ãªã‚‰ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«æ‰‹å‹•ç¢ºèªã‚’ä¾é ¼ã—ã€ç¢ºèªå¾Œã«æ¬¡ã®ã‚¿ã‚¹ã‚¯ã¸é€²ã‚€

 [æ¨©é™/å®‰å…¨ãƒãƒªã‚·ãƒ¼]
 - ã“ã®ã‚·ã‚¹ãƒ†ãƒ ã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã”ã¨ã«ãƒ„ãƒ¼ãƒ«æ¨©é™ãŒåˆ¶é™ã•ã‚Œã¾ã™ã€‚è¨±å¯ã•ã‚Œã¦ã„ãªã„ãƒ„ãƒ¼ãƒ«ã¯é¸ã°ãšã€å®Ÿè¡Œã‚‚ã§ãã¾ã›ã‚“ã€‚
 - éžã‚ªãƒ¼ãƒŠãƒ¼ï¼ˆè£½ä½œè€…ï¼‰ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«å¯¾ã—ã¦ã¯ã€ç ´å£Šãƒ»å‰Šé™¤ãƒ»ä¾µå…¥ãƒ»ãƒˆãƒ¼ã‚¯ãƒ³è¦æ±‚ãªã©ã®å±é™ºãªæ‰‹é †/ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®æç¤ºã‚’ã—ãªã„ã§ãã ã•ã„ã€‚
   ãã®å ´åˆã¯ã€Œã§ããªã„ã€æ—¨ã¨ã€å®‰å…¨ãªä»£æ›¿ï¼ˆä¸€èˆ¬çš„èª¬æ˜Žã€å…¬å¼æ‰‹é †ã€ã‚ªãƒ¼ãƒŠãƒ¼ã¸ã®ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰ã ã‘ã‚’ææ¡ˆã—ã¦ãã ã•ã„ã€‚

 [ãƒ‡ãƒã‚¤ã‚¹æƒ…å ±]
 {"[MOBILE] ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯ãƒ¢ãƒã‚¤ãƒ«ç«¯æœ«ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚å›žç­”ã¯ç°¡æ½”ã«ã¾ã¨ã‚ã€è¤‡é›‘ãªè¡¨ã‚„ãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆã¯é¿ã‘ã¦ãã ã•ã„ã€‚" if is_mobile else "[DESKTOP] ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯PCã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚è©³ç´°ãªè§£èª¬ã¨ãƒªãƒƒãƒãªãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆãŒå¯èƒ½ã§ã™ã€‚"}

  {memory_context}
  {guild_memory_context}
  {channel_memory_context}
  """

            # Prepend to prompt
            full_prompt = system_context.strip() + "\n\n" + prompt

            # [Vision Integration] Process Attachments & References
            vision_suffix = ""
            image_payloads = []

            try:
                # 1. Current Message
                # 1. Current Message
                # PERF: Unified GPT-5 Environment. Direct Image payload is sent.
                # We skip the captioning suffix to avoid redundant LLM calls and latency.
                if message.attachments:
                    # Only collect bytes/base64, don't trigger describe_media
                    _, imgs = await self.cog.vision_handler.process_attachments(message.attachments)
                    image_payloads.extend(imgs)

                # 2. Referenced Message (Reply) context
                if message.reference:
                    try:
                        if message.reference.cached_message:
                            ref_msg = message.reference.cached_message
                        else:
                            ref_msg = await message.channel.fetch_message(message.reference.message_id)

                        if ref_msg:
                            full_prompt += f"\n\n[REPLYING TO MESSAGE (Author: {ref_msg.author.display_name})]:\n{ref_msg.content or '(No Text)'}"
                            for embed in ref_msg.embeds:
                                if embed.url: full_prompt += f"\n[EMBED URL]: {embed.url}"

                            # Vision for References
                            if ref_msg.attachments:
                                 suffix, imgs = await self.cog.vision_handler.process_attachments(ref_msg.attachments, is_reference=True)
                                 vision_suffix += suffix
                                 image_payloads.extend(imgs)

                            if ref_msg.embeds:
                                 suffix, imgs = await self.cog.vision_handler.process_embeds(ref_msg.embeds, is_reference=True)
                                 vision_suffix += suffix
                                 image_payloads.extend(imgs)

                    except Exception as e:
                        logger.warning(f"Failed to fetch referenced message: {e}")

            except Exception as e:
                logger.error(f"Vision Processing Failed: {e}")
                # Fallback: Continue without vision data rather than crashing
                full_prompt += "\n[SYSTEM ERROR: Image processing failed. Proceeding with text only.]"

            # Append Vision Text Context
            full_prompt += vision_suffix

            # Prepare attachments for LLM (UnifiedClient expects 'attachments' list of dicts)
            # The structure from VisionHandler is already compatible or needs minor adapt?
            # UnifiedClient.chat expects 'attachments' argument.
            # But here we are building `messages` manually?
            # Wait, `endpoints.py` handles `attachments` argument.
            # In `ChatHandler`, we delegate to `core_client` or `unified_client`.

            # Update: ChatHandler calls `core_client.submit_message`.
            # We need to pass `image_payloads` to clean attachments.
            # Currently `chat_handler.py` uses `self.cog.unified_client` or `core_client`.
            # Let's see the call site further down.

            # Looking at previous code, `attachments` variable was created:
            # attachments = []
            # for att in message.attachments: ...

            # So I should assign `attachments = image_payloads`

            attachments = image_payloads

            # Send Request (Initial Handshake)
            # Fetch Context-Aware Tools (Discord Only)
            # Tool visibility is creator-locked: non-owner users only get safe allowlist tools.
            discord_tools = self.cog.get_context_tools("discord", user_id=message.author.id)

            # [RAG ROUTER] Analyze Intent & Select Tools
            # This reduces context usage and improves accuracy
            await status_manager.update_current("ðŸ” Intent Analysis (RAG)...")

            # [Clawdbot Feature] Vector Memory Retrieval (User + Guild Shared)
            guild_id_str = str(message.guild.id) if message.guild else None
            rag_context = await self.rag_handler.get_context(
                prompt=prompt,
                user_id=str(message.author.id),
                guild_id=guild_id_str
            )

            # Append RAG context to system prompt or user prompt?
            # Ideally User prompt to make it visible to the model as "Context"
            full_prompt_with_rag = f"{rag_context}\n{full_prompt}"

            # Select tools based on Platform Context
            selected_tools = await self.tool_selector.select_tools(
                prompt=prompt,
                available_tools=discord_tools,
                platform="discord",
                rag_context=rag_context,
                correlation_id=correlation_id
            )
            trace_event(
                "chat.tools_selected",
                correlation_id=correlation_id,
                available=len(discord_tools),
                selected=len(selected_tools),
                selected_names=[t.get("name") for t in selected_tools if isinstance(t, dict)],
            )

            # If router judges the request complex, force explicit plan-first behavior.
            route_meta = getattr(self.tool_selector, "last_route_meta", {}) or {}
            if route_meta.get("complexity") == "high":
                full_prompt_with_rag = (
                    "[ORCHESTRATION POLICY: COMPLEX TASK]\n"
                    "ã“ã®ä¾é ¼ã¯è¤‡é›‘ã§ã™ã€‚å¿…ãšæœ€åˆã«ã€ŽðŸ“‹ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè¡Œè¨ˆç”»ã€ã‚’çŸ­ãæç¤ºã—ã¦ã‹ã‚‰ã€"
                    "å¿…è¦ãªãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã‚’é–‹å§‹ã—ã¦ãã ã•ã„ã€‚\n\n"
                ) + full_prompt_with_rag

            # [SWARM] Optional high-complexity pre-orchestration
            if self.swarm.should_run(route_meta, prompt):
                await status_manager.add_timeline("Swarm: ã‚¿ã‚¹ã‚¯åˆ†è§£ä¸­")
                trace_event("swarm.triggered", correlation_id=correlation_id, route_meta=route_meta)
                try:
                    swarm_output = await self.swarm.run(
                        prompt=prompt,
                        rag_context=rag_context,
                        provider_id=str(message.author.id),
                        display_name=message.author.display_name,
                        context_binding=context_binding,
                        client_context=client_context,
                        correlation_id=correlation_id,
                    )
                    if swarm_output.get("ok") and swarm_output.get("summary"):
                        await status_manager.add_timeline("Swarm: çµæžœçµ±åˆå®Œäº†")
                        summary = swarm_output["summary"]
                        full_prompt_with_rag = (
                            "[SWARM PRE-ANALYSIS]\n"
                            f"{summary}\n\n"
                            "[Use the above as precomputed parallel analysis context.]\n\n"
                            + full_prompt_with_rag
                        )
                    else:
                        await status_manager.add_timeline("Swarm: Guardrailsã§åœæ­¢")
                except Exception as e:
                    logger.warning(f"Swarm pre-analysis failed: {e}")
                    await status_manager.add_timeline("Swarm: å¤±æ•— -> é€šå¸¸å‡¦ç†ã¸")
                    trace_event("swarm.exception", correlation_id=correlation_id, error=str(e))

            # If tools were filtered, log it
            if len(selected_tools) != len(discord_tools):
                logger.info(f"Tool Selection: {len(discord_tools)} -> {len(selected_tools)} tools")

            bot_cfg = getattr(self.bot, "config", None)
            preferred_model = getattr(bot_cfg, "openai_default_model", "gpt-5-mini")

            response = await core_client.send_message(
                content=full_prompt_with_rag,
                provider_id=str(message.author.id),
                display_name=message.author.display_name,
                conversation_id=None,
                idempotency_key=f"discord:{message.id}",
                context_binding=context_binding,
                attachments=attachments,
                stream=False, # User requested no streaming
                client_context=client_context,
                available_tools=selected_tools,  # Use RAG selected tools
                source="discord",
                llm_preference=preferred_model,
                correlation_id=correlation_id
            )

            if "error" in response:
                await status_manager.finish()
                await message.reply(f"âŒ Core API æŽ¥ç¶šã‚¨ãƒ©ãƒ¼: {response['error']}")
                trace_event("chat.core_send_error", correlation_id=correlation_id, error=response["error"])
                return

            run_id = response.get("run_id")
            await status_manager.set_task_state(1, "done", f"run_id={run_id[:8] if run_id else 'N/A'}")
            await status_manager.set_task_state(2, "running", "å¾…æ©Ÿä¸­")
            trace_event("chat.run_created", correlation_id=correlation_id, run_id=run_id)

            # 4. Process SSE Events (Streaming/Incremental Updates)
            full_content = ""
            model_name = "ORA Universal Brain"
            download_summaries = []
            tool_feedback_summaries = []
            if hasattr(self, "_plan_sent"):
                del self._plan_sent

            async for event in core_client.stream_events(run_id):
                ev_type = event.get("event")
                ev_data = event.get("data", {})

                if ev_type == "delta":
                    full_content += ev_data.get("text", "")

                    # [VISUALIZATION] Check if content is an Execution Plan (Relaxed Match)
                    has_plan_header = "Execution Plan" in full_content or "å®Ÿè¡Œè¨ˆç”»" in full_content
                    if has_plan_header and "1." in full_content and not hasattr(self, "_plan_sent"):
                        # Only send ONCE per run
                        msg_lines = full_content.split("\n")
                        plan_lines = [line.strip() for line in msg_lines if line.strip().startswith("1.") or line.strip().startswith("2.") or line.strip().startswith("3.") or line.strip().startswith("-")]

                        if plan_lines:
                             embed = discord.Embed(
                                 title="ðŸ¤– Harness Agent Execution Plan",
                                 description="\n".join(plan_lines),
                                 color=0x00ffff # Cyan (Codex Style)
                             )
                             embed.set_footer(text="OpenAI Codex Harness Architecture")
                             await message.reply(embed=embed)
                             self._plan_sent = True

                elif ev_type == "thought":
                    # Stream thoughts to a separate log or specific UI element
                    thought_text = ev_data.get("text", "")
                    logger.info(f"ðŸ§  [Harness Thought] {thought_text[:100]}...")
                    trace_event("chat.thought", correlation_id=correlation_id, run_id=run_id, text=thought_text)

                elif ev_type == "progress":
                    # Update status bar with Harness Progress
                    status_text = ev_data.get("status", "")
                    await status_manager.set_task_state(2, "running", status_text)
                    await status_manager.add_timeline(f"Progress: {status_text}")
                    trace_event("chat.progress", correlation_id=correlation_id, run_id=run_id, status=status_text)

                elif ev_type == "meta":
                     model_name = ev_data.get("model", model_name)

                elif ev_type == "dispatch":
                    # TOOL CALL detected!
                    tool_name = ev_data.get("tool")
                    tool_args = ev_data.get("args", {})
                    tool_call_id = ev_data.get("tool_call_id")
                    logger.info(f"ðŸš€ [Dispatch] CID: {correlation_id} | Tool: {tool_name}")
                    await status_manager.set_task_state(2, "running", f"{tool_name} å®Ÿè¡Œä¸­")
                    await status_manager.add_timeline(f"Dispatch: {tool_name}")
                    safe_args = self._sanitize_args_for_audit(tool_args if isinstance(tool_args, dict) else {})
                    trace_event(
                        "chat.dispatch",
                        correlation_id=correlation_id,
                        run_id=run_id,
                        tool=tool_name,
                        tool_call_id=tool_call_id,
                        args=tool_args if isinstance(tool_args, dict) else {},
                    )
                    asyncio.create_task(
                        self._notify_agent_activity(
                            "ðŸ§© Agent Dispatch",
                            f"CID: `{correlation_id}`\nRun: `{run_id}`\nTool: `{tool_name}`\nArgs: `{safe_args}`",
                            color=0x805AD5,
                        )
                    )

                    # Call ToolHandler (Handles music, imagine, tts, etc.)
                    # We pass the message context so it knows where to reply or join voice.
                    # [FIX] Use await instead of create_task to ensure SEQUENTIAL execution.
                    # This is critical for chains like "Screenshot -> Download -> Screenshot".
                    tool_result = await self.cog.tool_handler.handle_dispatch(
                         tool_name=tool_name,
                         args=tool_args,
                         message=message,
                         status_manager=status_manager,
                         correlation_id=correlation_id,
                         tool_call_id=tool_call_id,
                     )

                    if isinstance(tool_result, dict):
                        dl_meta = tool_result.get("download_meta")
                        if isinstance(dl_meta, dict):
                            summary = dl_meta.get("assistant_summary")
                            if isinstance(summary, str) and summary.strip():
                                download_summaries.append(summary.strip())
                        result_txt = tool_result.get("result")
                        if isinstance(result_txt, str) and result_txt.strip():
                            cleaned = result_txt.replace("[SILENT_COMPLETION]", "").strip()
                            if cleaned:
                                tool_feedback_summaries.append(cleaned)
                    elif isinstance(tool_result, str):
                        cleaned = tool_result.replace("[SILENT_COMPLETION]", "").strip()
                        if cleaned:
                            tool_feedback_summaries.append(cleaned)
                    trace_event(
                        "chat.tool_result",
                        correlation_id=correlation_id,
                        run_id=run_id,
                        tool=tool_name,
                        result_preview=str(tool_result)[:400],
                    )

                    # [FIX/AGENTIC] Submit Tool Result back to Core to break deadlock
                    if run_id:
                        logger.info(f"ðŸ“¤ Auto-submitting tool output for {tool_name} to Core...")
                        await core_client.submit_tool_output(
                            run_id=run_id,
                            tool_name=tool_name,
                            result=tool_result or "[Success]",
                            tool_call_id=tool_call_id,
                        )
                        await status_manager.add_timeline(f"Submitted: {tool_name}")
                        trace_event(
                            "chat.tool_submitted",
                            correlation_id=correlation_id,
                            run_id=run_id,
                            tool=tool_name,
                            tool_call_id=tool_call_id,
                        )
                        asyncio.create_task(
                            self._notify_agent_activity(
                                "âœ… Agent Tool Completed",
                                f"CID: `{correlation_id}`\nRun: `{run_id}`\nTool: `{tool_name}`\nStatus: submitted to Core",
                                color=0x2F855A,
                            )
                        )

                elif ev_type == "final":
                    final_text = extract_text_from_core_data(ev_data) or ""
                    if isinstance(final_text, str) and final_text.strip():
                        full_content = final_text
                    model_name = ev_data.get("model", model_name)
                    await status_manager.set_task_state(2, "done", "ãƒ„ãƒ¼ãƒ«é€£æºå®Œäº†")
                    await status_manager.set_task_state(3, "running", "æœ€çµ‚å›žç­”ã‚’æ•´å½¢ä¸­")
                    trace_event("chat.final_event", correlation_id=correlation_id, run_id=run_id, model=model_name)
                    break

                elif ev_type == "error":
                    await status_manager.set_task_state(2, "failed", ev_data.get("message", "error"))
                    await status_manager.set_task_state(3, "failed", "Coreã‚¨ãƒ©ãƒ¼")
                    await status_manager.finish()
                    await message.reply(f"âš ï¸ Core Error: {ev_data.get('message', 'Unknown error')}")
                    trace_event("chat.core_error_event", correlation_id=correlation_id, run_id=run_id, data=ev_data)
                    return

            # 5. Final Output Handover
            try:
                # [FIX] Flush any buffered files (Smart Bundling)
                if status_manager and hasattr(status_manager, "flush_files"):
                    try:
                        await status_manager.flush_files(message)
                    except Exception as e:
                        logger.error(f"Failed to flush files: {e}")
                        await message.reply(f"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«é€ä¿¡ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            finally:
                # Always finish status manager
                await status_manager.finish()

            if not full_content and not response.get("run_id"): # If we had tools, content might be empty but OK
                await message.reply("âŒ å¿œç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
                trace_event("chat.empty_response", correlation_id=correlation_id, run_id=run_id)
                return

            # If Core generated only a generic dispatch sentence, replace it with concrete download metadata summary.
            if download_summaries:
                generic_markers = [
                    "ä¿å­˜å‡¦ç†ã‚’discordã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã«ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒ",
                    "ä¿å­˜ãŒå®Œäº†ã—ãŸã‚‰",
                    "ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒã—ã¾ã—ãŸ",
                ]
                low = (full_content or "").lower()
                if (not full_content.strip()) or any(m in low for m in generic_markers):
                    full_content = "\n".join(download_summaries[-2:])

            # General fallback: if core final is empty, surface concrete tool feedback.
            if not (full_content or "").strip() and tool_feedback_summaries:
                uniq = []
                for t in tool_feedback_summaries:
                    if t not in uniq:
                        uniq.append(t)
                full_content = "\n".join(uniq[-2:])

            if not (full_content or "").strip():
                full_content = "ãƒ„ãƒ¼ãƒ«å‡¦ç†ã¯å®Ÿè¡Œã•ã‚Œã¾ã—ãŸãŒã€æœ€çµ‚ãƒ†ã‚­ã‚¹ãƒˆå¿œç­”ãŒç©ºã§ã—ãŸã€‚å¿…è¦ãªã‚‰çµæžœã‚’å†è¡¨ç¤ºã—ã¾ã™ã€‚"

            # Send as Embed Cards
            # Split if > 4000 chars
            remaining = full_content
            await status_manager.set_task_state(3, "done", "å›žç­”å®Œäº†")
            while remaining:
                chunk = remaining[:4000]
                remaining = remaining[4000:]
                embed = EmbedFactory.create_chat_embed(chunk, model_name=model_name)
                await message.reply(embed=embed)
            trace_event(
                "chat.reply_sent",
                correlation_id=correlation_id,
                run_id=run_id,
                model=model_name,
                reply_length=len(full_content or ""),
            )

            # 6. Post-Process Actions (Voice, etc.)
            # [MEMORY UPDATE] Inject AI response into MemoryCog buffer
            try:
                memory_cog = self.bot.get_cog("MemoryCog")
                if memory_cog:
                    # Note: MemoryCog expects visibility scope; use same public/private decision as MemoryCog does.
                    is_pub = memory_cog.is_public(message.channel) if hasattr(memory_cog, "is_public") else True
                    asyncio.create_task(
                        memory_cog.add_ai_message(
                            user_id=message.author.id,
                            content=full_content,
                            guild_id=message.guild.id if message.guild else None,
                            channel_id=message.channel.id,
                            channel_name=message.channel.name if hasattr(message.channel, "name") else "DM",
                            guild_name=message.guild.name if message.guild else "Direct Message",
                            is_public=is_pub,
                        )
                    )
            except Exception as e:
                logger.warning(f"Failed to update MemoryCog: {e}")

            # Check if user is in VC and if we should speak
            if is_voice:
                # [REQUESTED FEATURE] Suppress AI speech if this channel is an Auto-Read channel
                # User wants "Reading Bot" behavior where AI text response is just text, unless specifically asked?
                # Or simply "Don't read AI response".
                should_speak = True
                if message.guild and hasattr(self.bot, "voice_manager"):
                    auto_channel_id = self.bot.voice_manager.auto_read_channels.get(message.guild.id)
                    if auto_channel_id == message.channel.id:
                        should_speak = False

                # [FIX] Accessed via bot instance as it's a shared resource now
                if should_speak and hasattr(self.bot, "voice_manager"):
                    await self.bot.voice_manager.play_tts(message.author, full_content)
                else:
                    logger.warning("VoiceManager not found on Bot instance.")

        except Exception as e:
            logger.error(f"Core API Delegation Failed: {e}", exc_info=True)
            await status_manager.finish()
            await message.reply(f"ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼: {e}")
            trace_event("chat.exception", correlation_id=correlation_id, error=str(e))

    # --- END OF THIN CLIENT ---
